{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import sys\n",
    "import xlsxwriter as xl\n",
    "import random\n",
    "from random import randint\n",
    "import pandas\n",
    "from pandas import read_csv\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules for visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functional_modules import feature_computation_module as fc\n",
    "\n",
    "from functional_modules import file_locations_module as flocate\n",
    "from functional_modules import pca_module\n",
    "\n",
    "from functional_modules import DTreeModel as dtree\n",
    "from functional_modules import GaussianNBmodel as gauss\n",
    "from functional_modules import KNeighborModel as knbr\n",
    "from functional_modules import LDAmodel as lda\n",
    "from functional_modules import LogRegModel as log\n",
    "from functional_modules import RandForestModel as rf\n",
    "from functional_modules import SVCmodel as svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_count = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VLAD(X, visualDictionary):\n",
    "    predictedLabels = visualDictionary.predict(X)\n",
    "    centers = visualDictionary.cluster_centers_\n",
    "    labels = visualDictionary.labels_\n",
    "    k = visualDictionary.n_clusters\n",
    "\n",
    "    m,d = X.shape\n",
    "    V=np.zeros([k,d])\n",
    "    #computing the differences\n",
    "\n",
    "    # for all the clusters (visual words)\n",
    "    for i in range(k):\n",
    "        # if there is at least one descriptor in that cluster\n",
    "        if np.sum(predictedLabels==i)>0:\n",
    "            # add the diferences\n",
    "            V[i]=np.sum(X[predictedLabels==i,:]-centers[i],axis=0)\n",
    "\n",
    "    V = V.flatten()\n",
    "    # power normalization, also called square-rooting normalization\n",
    "    V = np.sign(V)*np.sqrt(np.abs(V))\n",
    "\n",
    "    # L2 normalization\n",
    "    V = V/np.sqrt(np.dot(V,V))\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVLADDescriptors(path,visualDictionary,low,high,n):\n",
    "    descriptors=list()\n",
    "    idImage =list()\n",
    "    for i in range(n):\n",
    "        print('Data-{}'.format(i+1))\n",
    "        img = np.load(path + 'data{}.npy'.format(i+1), allow_pickle=True)\n",
    "        l, h = fc.get_high_low_gray_level(img, i+1)\n",
    "        img = fc.change_image_dynamic_range(img, i+1, l, h)\n",
    "\n",
    "        final_des = list()\n",
    "        for j in range(low,high):\n",
    "            cv2.imwrite('photo.jpg',img[j])\n",
    "            img1 = cv2.imread('photo.jpg',0)\n",
    "            kp, des = describeORB(img1)\n",
    "            \n",
    "            if des is not None:\n",
    "                r = des.shape[0]\n",
    "                c = des.shape[1]\n",
    "                row = list()\n",
    "                if r>=feature_count:\n",
    "                    for k in range(feature_count):\n",
    "                        for m in range(c):\n",
    "                            row.append(des[k,m])\n",
    "                else:\n",
    "                    for k in range(r):\n",
    "                        for m in range(c):\n",
    "                            row.append(des[k,m])\n",
    "\n",
    "                    for k in range(feature_count-r):\n",
    "                        for m in range(c):\n",
    "                            row.append(0)\n",
    "\n",
    "            row = np.asarray(row)\n",
    "            final_des.append(row)\n",
    "        else:\n",
    "            row = list()\n",
    "\n",
    "            for k in range(feature_count):\n",
    "                    for m in range(32):\n",
    "                        row.append(0)\n",
    "\n",
    "            row = np.asarray(row)\n",
    "            final_des.append(row)\n",
    "\n",
    "        final_des = np.asarray(final_des)\n",
    "        print('des calculated..')\n",
    "\n",
    "        print('VLAD-method called ..')\n",
    "        v=VLAD(final_des,visualDictionary)\n",
    "        print('VLAD recieved...')\n",
    "        descriptors.append(v)\n",
    "        idImage.append(i)\n",
    "\n",
    "    #list to array\n",
    "    descriptors = np.asarray(descriptors)\n",
    "    return descriptors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  kMeansDictionary(training, k):\n",
    "    '''\n",
    "    :param training: Descriptors obtained from SIFT,ORB, or something else\n",
    "    :param k: number of visual words or clusters..\n",
    "    :return: returns the words\n",
    "    '''\n",
    "    #K-means algorithm\n",
    "    print('Inside kMeansDictionary function.')\n",
    "    est = KMeans(n_clusters=k,init='k-means++',tol=0.0001,verbose=1).fit(training)\n",
    "    #centers = est.cluster_centers_\n",
    "    #labels = est.labels_\n",
    "    #est.predict(X)\n",
    "    print('Exiting kMeansDictionary')\n",
    "    return est\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describeORB( image):\n",
    "    #An efficient alternative to SIFT or SURF\n",
    "    #doc http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_orb/py_orb.html\n",
    "    #ORB is basically a fusion of FAST keypoint detector and BRIEF descriptor\n",
    "    #with many modifications to enhance the performance\n",
    "    orb=cv2.ORB_create()\n",
    "    kp, des=orb.detectAndCompute(image, None) #Image should be .jpeg format\n",
    "    return kp,des\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_descriptors(loc, low, high, n, descriptors, group):\n",
    "    '''\n",
    "    :param loc: Where the files are\n",
    "    :param low: lowest number of the slice that would be selected - 40\n",
    "    :param high: highest number of the slice that would be selected - 150+1\n",
    "    :param n: numbers of files in the location\n",
    "    :return: a list of descriptors ...\n",
    "    '''\n",
    "    size = 0\n",
    "    h_len = 0\n",
    "    for i in range(n):\n",
    "        print('{}-Data-{}'.format(group, i+1))\n",
    "        img = np.load(loc+'data{}.npy'.format(i+1), allow_pickle=True)\n",
    "        l, h = fc.get_high_low_gray_level(img, i+1)\n",
    "        img = fc.change_image_dynamic_range(img, i+1, l, h)\n",
    "\n",
    "        final_des = list()\n",
    "        for j in range(low, high):\n",
    "            cv2.imwrite('photo.jpg', img[j])\n",
    "            img1 = cv2.imread('photo.jpg', 0)\n",
    "            kp,des = describeORB(img1)\n",
    "            \n",
    "            #print(des.shape)\n",
    "            '''\n",
    "            if len(kp) > h_len:\n",
    "                h_len = len(kp)\n",
    "                print(h_len)\n",
    "            '''\n",
    "\n",
    "            if des is not None:\n",
    "                r = des.shape[0]\n",
    "                c = des.shape[1]\n",
    "                #size = size + 50*c\n",
    "                row = list()\n",
    "                if r>=feature_count:\n",
    "                    for k in range(feature_count):\n",
    "                        for m in range(c):\n",
    "                            row.append(des[k,m])\n",
    "                else:\n",
    "                    for k in range(r):\n",
    "                        for m in range(c):\n",
    "                            row.append(des[k,m])\n",
    "\n",
    "                    for k in range(feature_count-r):\n",
    "                        for m in range(c):\n",
    "                            row.append(0)\n",
    "\n",
    "                row = np.asarray(row)\n",
    "                final_des.append(row)\n",
    "            else:\n",
    "                row = list()\n",
    "                for k in range(feature_count):\n",
    "                    for m in range(32):\n",
    "                        row.append(0)\n",
    "\n",
    "                row = np.asarray(row)\n",
    "                final_des.append(row)\n",
    "\n",
    "        final_des = np.asarray(final_des)\n",
    "        descriptors.append(final_des)\n",
    "\n",
    "        #print('Total Size of Descriptors: {} MB'.format(size/128318))\n",
    "        #c = input('Enter for next: ')\n",
    "\n",
    "    #descriptors = list(itertools.chain.from_iterable(descriptors)) #Flatten\n",
    "    #descriptors = np.asarray(descriptors)\n",
    "    #print(h_len)\n",
    "    return descriptors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### 1. Making Ready for All_features ########\n",
    "def get_all_descriptors(low, high):\n",
    "    des = list()\n",
    "    total = 0\n",
    "    print('#######################')\n",
    "    loc = 'E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\AD_mainNPY\\\\'\n",
    "    n = 54\n",
    "    total += n\n",
    "    des = all_descriptors(loc, low, high, n, des, 'AD')\n",
    "    #beeper()\n",
    "    #input('AD complete. Enter to continue >>')\n",
    "\n",
    "\n",
    "    print('#######################')\n",
    "    loc = 'E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\CN_mainNPY\\\\'\n",
    "    n = 54#115\n",
    "    total += n\n",
    "    des = all_descriptors(loc, low, high, n, des, 'CN')\n",
    "    #beeper()\n",
    "    #input('CN complete. Enter to continue >>')\n",
    "\n",
    "\n",
    "    print('#######################')\n",
    "    loc = 'E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\MCI_mainNPY\\\\'\n",
    "    n = 54#133\n",
    "    total += n\n",
    "    des = all_descriptors(loc, low, high, n, des, 'MCI')\n",
    "    #beeper()\n",
    "    #input('MCI complete. Enter to continue >>')\n",
    "\n",
    "\n",
    "    des = list(itertools.chain.from_iterable(des)) #Flatten\n",
    "    des = np.asarray(des).astype('uint8')\n",
    "\n",
    "    np.save(\n",
    "        f'E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\\\vlad\\\\VLAD_{feature_count}_uint_feat.npy', des)\n",
    "\n",
    "    print()\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# 2. Making Visual Words #############\n",
    "def get_visual_dict():\n",
    "    vlad_data_file = np.load(f\"E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\\\vlad\\VLAD_{feature_count}_uint_feat.npy\", allow_pickle=True)\n",
    "    print(vlad_data_file.shape)\n",
    "    print(np.max(vlad_data_file))\n",
    "    print(vlad_data_file.dtype)\n",
    "    print(vlad_data_file[0, -1])\n",
    "    visualDict = kMeansDictionary(vlad_data_file, 256)\n",
    "    print('Visual Dictionary obtained.')\n",
    "\n",
    "    model_file = f'E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\\\vlad\\\\KMean{feature_count}_model.sav'\n",
    "    pickle.dump(visualDict, open(model_file, 'wb'))\n",
    "    print('Visual Dictionary model saved.')\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# 3. Getting the VLAD descriptors #############\n",
    "def get_vlad_desc(low, high):\n",
    "    n = 54\n",
    "\n",
    "    ad_loc = 'E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\AD_mainNPY\\\\'\n",
    "    cn_loc = 'E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\CN_mainNPY\\\\'\n",
    "    mci_loc = 'E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\MCI_mainNPY\\\\'\n",
    "\n",
    "    visualDict = pickle.load(open(f\"E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\\\vlad\\\\KMean{feature_count}_model.sav\", 'rb'))\n",
    "\n",
    "    '''\n",
    "    vlad_ad = getVLADDescriptors(ad_loc, visualDict, low, high, n)\n",
    "\n",
    "    np.save(f'E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\\\vlad\\\\vlad{feature_count}_ad.npy', vlad_ad)\n",
    "\n",
    "    print('--- AD complete ---')\n",
    "    #input('AD complete. Enter to continue >>')\n",
    "    '''\n",
    "\n",
    "    \n",
    "    ''''''\n",
    "    vlad_cn = getVLADDescriptors(cn_loc, visualDict, low, high, n)\n",
    "\n",
    "    np.save(f'E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\\\vlad\\\\vlad{feature_count}_cn.npy', vlad_cn)\n",
    "\n",
    "    print()\n",
    "    input('CN complete. Enter to continue >>')\n",
    "    ''''''\n",
    "\n",
    "    ''''''\n",
    "    vlad_mci = getVLADDescriptors(mci_loc, visualDict, low, high, n)\n",
    "\n",
    "    np.save(\n",
    "        f'E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\\\vlad\\\\vlad{feature_count}_mci.npy', vlad_mci)\n",
    "\n",
    "    print()\n",
    "    input('MCI complete. Enter to continue >>')\n",
    "    ''''''\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vlads():\n",
    "    # param target: 1 for AD, 2 for CN and 3 for MCI\n",
    "\n",
    "    adv = f\"E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\\\vlad\\\\vlad{feature_count}_ad.npy\"\n",
    "\n",
    "    cnv = f\"E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\\\vlad\\\\vlad{feature_count}_cn.npy\"\n",
    "\n",
    "    mciv = f\"E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\\\vlad\\\\vlad{feature_count}_mci.npy\"\n",
    "\n",
    "    adv = np.load(adv, allow_pickle=True)\n",
    "    print('AD-VLAD loaded')\n",
    "    cnv = np.load(cnv, allow_pickle=True)\n",
    "    print('CN-VLAD loaded')\n",
    "    mciv = np.load(mciv, allow_pickle=True)\n",
    "    print('MCI-VLAD loaded')\n",
    "\n",
    "    n = adv.shape[0]\n",
    "\n",
    "    ad_t = np.full((n,), 1, dtype='uint8')\n",
    "    print('AD-VLAD-T generated')\n",
    "    cn_t = np.full((n,), 2, dtype='uint8')\n",
    "    print('AD-VLAD-T generated')\n",
    "    mci_t = np.full((n,), 3, dtype='uint8')\n",
    "    print('AD-VLAD-T generated')\n",
    "\n",
    "    adv_t = np.column_stack((adv, ad_t))\n",
    "    print('AD-T appended')\n",
    "    cnv_t = np.column_stack((cnv, cn_t))\n",
    "    print('CN-T appended')\n",
    "    mciv_t = np.column_stack((mciv, mci_t))\n",
    "    print('MCI-T appended')\n",
    "\n",
    "    vlad_all_cases = np.concatenate((adv_t, cnv_t, mciv_t), axis=0)\n",
    "    print('All cases merged')\n",
    "\n",
    "    np.save(f'E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\\\vlad\\\\vlad_all_cases_{feature_count}.npy', vlad_all_cases)\n",
    "    print('Saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Time elapsed- 00:01:22\n"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "low = 40\n",
    "high = 151\n",
    "# step - 1\n",
    "#get_all_descriptors(low, high) # complete - 50, 100, 200 \n",
    "# step - 2\n",
    "#get_visual_dict()   # complete - 50, 100\n",
    "# step - 3\n",
    "#get_vlad_desc(low,high)\n",
    "# step - 4\n",
    "#merge_vlads()\n",
    "e = int(time.time() - start_time)\n",
    "#print('Time elapsed- {:02d}:{:02d}:{:02d}'.format(e //3600, (e % 3600 // 60), e % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Classify-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_path):\n",
    "    all_data = np.load(data_path, allow_pickle=True)\n",
    "    print('Data shape >> ', all_data.shape)\n",
    "    all_X = all_data[:, :-1]\n",
    "    all_Y = all_data[:, -1]\n",
    "    return all_X, all_Y\n",
    "\n",
    "def create_excel(excel_loc, title, classifier):\n",
    "    headers = classifier.headers\n",
    "\n",
    "    outWorkbook = xl.Workbook(excel_loc+title+'.xlsx')\n",
    "    outSheet = outWorkbook.add_worksheet()\n",
    "    L = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    for i in range(len(headers)):\n",
    "        outSheet.write(L[i]+'1', headers[i])\n",
    "    print('Excel created >> '+excel_loc+title+'.xlsx')\n",
    "    return outWorkbook, outSheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, X, Y, book, sheet, line=1, serial=1, doCompo=False):\n",
    "\n",
    "    combo_list = classifier.combos  # call function here\n",
    "    number_of_combos = len(combo_list)\n",
    "    print('Number of total combinations >> ', number_of_combos)\n",
    "    shuffle = 75\n",
    "    combo_list = random.choices(combo_list, k=shuffle)\n",
    "    print('Number of selected combos %d'%shuffle)\n",
    "    successful = []\n",
    "    headers = classifier.headers\n",
    "\n",
    "    print('Processing compo #', serial)\n",
    "    x = X\n",
    "    if doCompo:\n",
    "        x = pca_module.applyPCA(X, serial)\n",
    "        print('PCA successfully applied for component #%d' % (serial+1))\n",
    "    \n",
    "    success = 0\n",
    "    best_score = 0\n",
    "    fail = 0\n",
    "    scores = []\n",
    "\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(x, Y, test_size=0.3)\n",
    "\n",
    "    for c in range(shuffle):\n",
    "        print('Entering combo #', c+1)\n",
    "        try:\n",
    "            print(combo_list[c])\n",
    "            score_model = classifier.make_model(c, train_X, train_Y, test_X, test_Y)\n",
    "            #time.sleep(1)\n",
    "            print('for Compo #{} - Combo #{} - #{} Combos Successful!\\nScore: {}'.format(serial, c+1, success+1, score_model))\n",
    "            success += 1\n",
    "            #successful.append(combo_list[c])\n",
    "            if score_model > best_score:\n",
    "                print('New highest accuracy:',score_model, '>', best_score)                \n",
    "                best_score = score_model\n",
    "                scores.append(best_score)\n",
    "                limit = len(headers) - 3\n",
    "                for i in range(len(headers)-3):\n",
    "                    sheet.write(line, i, combo_list[c][i])\n",
    "\n",
    "                sheet.write(line, (len(headers)-1), best_score*100)\n",
    "                sheet.write(line, (len(headers)-2), serial)\n",
    "                sheet.write(line, (len(headers)-3), best_score)\n",
    "                print('Line #{} --- Component #{}'.format(line, serial))\n",
    "                line += 1\n",
    "        except:\n",
    "            print('Compo #',serial,' - Combo failed at #', c+1)\n",
    "            fail += 1\n",
    "        print('Exiting compo #%d - combo #%d'% (serial, c+1))\n",
    "        print()\n",
    "\n",
    "    print('Compo %d - all done.'%serial)\n",
    "    print('Total combinations: ', number_of_combos)\n",
    "    print('Total success: ', success)\n",
    "    print('Total failure:', fail)\n",
    "    #print(successful)\n",
    "    #input('ENTER to continue...')\n",
    "    return line,scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_glcm(model, book, sheet, limit, path):\n",
    "    X, Y = prepare_data(path)\n",
    "    line = 1\n",
    "    scores = []\n",
    "    for serial in range(1,limit):\n",
    "        line, best_scores = train_model(model, X, Y, book, sheet, line, serial, True)\n",
    "        scores.append(best_scores)\n",
    "        print('Serial #', serial, 'done.')\n",
    "    print(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_hog(model, book, sheet, limit):    \n",
    "    line = 1\n",
    "    scores = []\n",
    "    for serial in range(1,limit):\n",
    "        path = flocate.HOG_all_case_feats_form.format(serial)\n",
    "        X, Y = prepare_data(path)\n",
    "        line, best_scores = train_model(model, X, Y, book, sheet, line, serial, False)\n",
    "        scores.append(best_scores)\n",
    "        print('Serial #', serial, 'done.')\n",
    "    print(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_vlad(model, book, sheet, path):  \n",
    "    X, Y = prepare_data(path)\n",
    "    print('Data distribution complete.')\n",
    "    scores = []\n",
    "    line, scores = train_model(model, X, Y, book, sheet)\n",
    "    print(scores)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Excel created >> E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\FiftyFour\\excels\\\\LDA__vlad100.xlsx\nData shape >>  (162, 819201)\nData distribution complete.\nNumber of total combinations >>  210\nNumber of selected combos 75\nProcessing compo # 1\nEntering combo # 1\n['lsqr', 'auto', False, 0.2]\nCompo # 1  - Combo failed at # 1\nExiting compo #1 - combo #1\n\nEntering combo # 2\n['lsqr', 0.75, True, 0.1]\nCompo # 1  - Combo failed at # 2\nExiting compo #1 - combo #2\n\nEntering combo # 3\n['lsqr', 0.5, True, 0.001]\nCompo # 1  - Combo failed at # 3\nExiting compo #1 - combo #3\n\nEntering combo # 4\n['lsqr', 'auto', True, 0.2]\n"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#model = dtree\n",
    "#model = gauss\n",
    "#model = knbr\n",
    "#model = svc\n",
    "#model = rf     # time consuming - 36 combos\n",
    "model = lda    # time consuming - 210 combos\n",
    "#model = log     # time consuming - 336 //924 combos\n",
    "    \n",
    "#title = model.title+'_glcm'\n",
    "#title = model.title+'_hog'\n",
    "n = 100\n",
    "title = model.title +f'_vlad{n}'\n",
    "\n",
    "excel_loc = r'E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\FiftyFour\\excels\\\\'\n",
    "book, sheet = create_excel(excel_loc, title, model)\n",
    "\n",
    "#limit = 161\n",
    "\n",
    "# function for handling glcm\n",
    "#glcm_path = r\"E:\\THESIS\\\\ADNI_data\\\\ADNI1_Annual_2_Yr_3T_306_WORK\\FiftyFour\\GLCM54feats54.npy\"\n",
    "#classify_glcm(model, book, sheet, limit, glcm_path)\n",
    "    \n",
    "# function for handling hog\n",
    "#classify_hog(model, book, sheet, limit)\n",
    "\n",
    "# function for handling vlad\n",
    "vlad_path = f\"E:\\THESIS\\ADNI_data\\ADNI1_Annual_2_Yr_3T_306_WORK\\\\vlad\\\\vlad_all_cases_{n}.npy\"\n",
    "#csv_file_path = r\"F:\\\\AI-ML-DL_works\\Irist-dataset-ML\\iris-Copy.csv\"\n",
    "classify_vlad(model, book, sheet, vlad_path)\n",
    "#classify_vlad(model, book, sheet, csv_file_path)\n",
    "    \n",
    "book.close()\n",
    "print()\n",
    "\n",
    "e = int(time.time() - start_time)\n",
    "print('{:02d}:{:02d}:{:02d}'.format(e // 3600, (e % 3600 // 60), e % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}